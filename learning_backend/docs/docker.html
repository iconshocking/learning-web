<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Docker</title>
    <style>
      p + p {
        margin-top: 1em;
      }

      .background-code {
        code {
          background-color: #e2e1e1;
        }
      }

      li + li {
        margin-top: 1em;
      }

      h2 {
        text-decoration: underline;
      }

      h3 {
        margin-top: 2.25rem;
      }
    </style>
  </head>
  <body>
    <main class="background-code">
      <h1>Docker</h1>
      <h2>The Pieces</h2>
      <section>
        <ul>
          <li>
            <strong>Dockerfiles:</strong> define the instructions that are run to compile an image.
          </li>
          <li>
            <strong>Images:</strong> the compiled and collected libraries and files generated by
            building a Dockerfile (essentially a file system snapshot). They are <em>immutable</em>,
            and also platform-dependent since any binaries have been generated.

            <p>
              Note that while an image is immutable, successive images built from an unchanged
              Dockerfile may differ if dependencies and image versions in the Dockerfile are not
              hardcoded.
            </p>
          </li>
          <li>
            <p>
              <strong>Containers:</strong> the instanced versions of images. These have a lifecycle
              and can be started/stopped. Since images are immutable, containers can statically
              reference the image they were created from.
            </p>
            <p>
              NOTE: Docker containers should only run one process, but a process manager (like
              Supervisor) can be used to manage multiple processes within a single container.
            </p>
          </li>
        </ul>
        <p>
          A useful analogy to OOP is the Dockerfile is ths class <strong>file</strong>, the image is
          the compiled class <strong>object</strong>, and the container is the class
          <strong>instance</strong>.
        </p>
        <p>
          NOTE: There are also <strong>volumes</strong>, which allow for persisted storage between
          containers. Volumes can either be created by docker or point directly to directories in
          the host machine. (Bind mounts are the same thing as host-path volumes and have been
          mostly replaced by them.)
        </p>
      </section>

      <h2>Performance & Stability</h2>
      <section>
        <p>
          Docker containers are natively performant and have trivial overhead (unlike VMs) because
          they are just Linux namespaces under-the-hood, which isolate the container processes and
          files.
        </p>

        <p>
          Container orchestration tools manage the lifecycle of a container, scaling and restarting
          as needed. Docker compose has some simple support for this, but often is not robust enough
          for production.
        </p>
      </section>

      <h2>Commands</h2>
      <section>
        <ul>
          <li><code>docker build</code>: builds an image from a Dockerfile</li>
          <li><code>docker create [image_name]</code>: creates a container of an image</li>
          <li>
            <code>docker start [container_name]</code>: runs the <code>CMD</code> instruction for
            that container's image
          </li>
          <li>
            <code>docker run [image_name]</code>: creates <em>and</em> starts a container of an
            image
          </li>
          <li>
            <code>docker compose up</code>: creates, networks, and runs the containers described in
            <code>compose.yaml</code>.
            <p>Useful options:</p>
            <ul>
              <li><code>docker compose --build</code>: rebuilds the images</li>
              <li>
                <code>docker compose build --no-cache</code>: build without using the cache (cannot
                be set as an option on <code>docker compose up</code> unfortunately)
              </li>
              <li><code>docker compose -d</code>: runs in the background (detaches)</li>
              <li>
                <code>docker compose up --no-deps --build [service name array]</code> rebuild images
                for specified services and create their containers, disregarding other services in
                <code>depends on</code> attribute in compose file
              </li>
            </ul>
          </li>
          <li>
            <code>docker compose down [services_array]</code>: stops and destroys the containers for
            the listed services
          </li>
        </ul>
      </section>

      <h2>Networking</h2>
      <section>
        <p>
          Containers have networking by default and can making <em>outgoing</em> connections (e.g.,
          they can access the internet). However, they are isolated from
          <em>incoming</em> connections by default.
        </p>

        <p>There are two types of networks that a container can use:</p>
        <ul>
          <li>
            <strong>bridge:</strong> containers are connected via a local network but must be told
            explicitly which ports to forward from the host to the container. Bridge networks are
            isolated from each other, so containers must be connected to multiple bridge networks if
            they want to communicate across more than one.
          </li>
          <li>
            <p>
              <strong>host:</strong> the containers' network is not isolated from the host, so the
              port bound in the container is actually just the host's port. Therefore, the host or
              any container using 'host' networking can communiate with the container.
            </p>
            <p>
              NOTE: host networks are more performant since there is no IP lookup required, but are
              much more limiting, preventing multiple containers of the same image being spun up on
              the same host since they are all trying to access the same ports.
            </p>
          </li>
        </ul>

        <p>
          Containers can communicate to each other via a Docker <strong>network</strong>, a local
          network between the containers. A network is created in three ways:
        </p>
        <ul>
          <li>
            Docker compose creates a new bridge network automatically for containers within the same
            compose file
          </li>
          <li><code>docker network create</code> creates a new bridge network</li>
          <li>
            Containers created without using compose files are all connected to the 'default'
            bridge, which is <em>not</em> recommened.
            <p>
              <strong>NOTE:</strong> do NOT use the default bridge (less isolated and cannot be
              disconnected from without recreating the container)
            </p>
          </li>
        </ul>

        <p>
          <strong>Container-to-container communication:</strong> containers on the same network have
          all ports exposed to each other. Each container's host name is the container's name (when
          using Docker compose or user-defined bridge networks).
        </p>
        <p>
          <strong>Host-to-container communication:</strong> requires port "publishing", which is
          port forwarding, where a port from the host is forwarded to a port in the Docker network
        </p>

        <p>
          Examples of publishing ports (these are options for a <code>docker run</code> command):
        </p>
        <ul>
          <li>
            <code>-p 8080:80</code> maps port 8080 on the Docker host to TCP port 80 in the
            container
          </li>
          <li>
            <code>-p 8080:80/udp</code> maps port 8080 on the Docker host to UDP port 80 in the
            container
          </li>
          <li>
            <code>-p 8080:80/tcp -p 8080:80/udp</code> maps TCP port 8080 on the Docker host to TCP
            port 80 in the container, and maps UDP port 8080 on the Docker host to UDP port 80 in
            the container
          </li>
          <li>
            <code>-p 192.168.1.100:8080:80</code> maps port 8080 on the Docker host IP 192.168.1.100
            to TCP port 80 in the container
          </li>
        </ul>

        <p>
          Lcal-network-only addresses can be useful for safe restriction of ports. For example:
          <code>docker run -p 127.0.0.1:8080:80 [container_name]</code> will only expose that
          container port to the <em>Docker host</em>. But if no IP is provided, the container port
          is exposed to any outside network communication that hits port 8080 on the Docker host.
          The distinction in the provided example is that the Docker host must initiate the
          connection to the container, via explicit loopback address forwarding instead of the
          container being called directly.
        </p>

        <p>
          <code>EXPOSE</code> (in Dockerfile) and <code>expose</code> (in Docker compose file) do
          not actually <em>DO</em> anything, but are documentation stating which ports to
          communicate with for that container.
        </p>

        <p>
          A container can be connected to multiple networks using
          <code>docker network connect</code>, but Docker compose will default to creating a single
          network for containers defined in a single compose file.
        </p>

        <p>
          You can inspect networks using <code>docker network ls</code> and
          <code>docker inspect [network-name]</code>. You'll notice that the network defines its own
          gateway, and each container connected to the network will have its own IP & MAC addresses
          (these IP addresses will always be in the local-network-only ranges.)
        </p>
      </section>

      <h2>Environment Variables</h2>
      <section>
        <small
          >HINT: Use <code>docker compose config</code> to print the finalized compose config that
          will be run after all substitutions and merges.</small
        >

        <h3>ARG vs ENV</h3>
        <section>
          <p>
            <strong>arg/ARG</strong> variables are passed <em>only</em> during image building and
            are <em>not</em> passed to the container (which means they cannot be used in the
            <code>RUN</code> or <code>ENTRYPOINT</code> instructions).
          </p>

          <p>
            <strong>Environment</strong> variables are accessible during image building
            <em>and</em> within the container runtime, <strong>BUT</strong> you cannot pass
            environment variables <em>into</em> a Dockerfile. Therefore, to use environment
            variables in the Dockerfile itself, you must either hardcode the
            <code>ENV</code> variable in the Dockerfile or set the ENV value via an
            <code>ARG</code> variable, which will be auto-populated if the same <code>arg</code> is
            defined in the docker compose file.
          </p>

          <p>
            NOTE: you cannot export environment variables into the buildtime environment from the
            <code>RUN</code> commands because each instruction in a Dockerfile runs in its own
            layer, where the files of the previous layer are copied over but not any env variables /
            state. Therefore, you would need to run multiple commands in a single
            <code>RUN</code> instruction to use custom environment variables within those commands.
          </p>

          <p>
            NOTE: <em>no</em> variables are accessible from within the compose file itself (there is
            substitution though).
          </p>
        </section>

        <h3>Accessing Variables in the Dockerfile</h3>
        <section>
          <p>
            <code>ARG variable=[default_value_if_absent]</code> sets the value of a variable if the
            variable has not been passed a value via some other mechanism.
          </p>
          <p>
            <code>ENV</code> must be set manually (or via computation) or via an
            <code>ARG</code> variable.
          </p>
          <p>
            <strong>NOTE:</strong> any variable used in the Dockerfile outside of
            <code>RUN/ENTRYPOINT</code> <code>must</code> be declared via
            <code>ENV/ARG</code> before use, so if the value is being passed by another mechanism,
            write <code>ARG/ENV variable</code> to use that provided value.
          </p>
          <p>
            NOTE: you must declare variable in the stage of a multi-stage build they are referenced,
            even if already declared in an earlier stage.
          </p>
          <p>
            You can also use <code>${[variable]:-/+[value]}</code>, when referencing variable
            values, to set <code>value</code> as the result according to whether the variable is set
            or not.
          </p>
        </section>

        <h3>Inject variables</h3>
        <section>
          <p>
            <code>.env</code> file or files set using
            <code>docker compose --env-file [file array]</code> are <em>ONLY</em> relevant to
            <em>compose</em> files; variables defined here will be automatically loaded by Docker
            compose before running any CLI command for a compose file and substituted into the
            compose file, wherever there is <code>${VARIABLE_NAME}</code> syntax. These are
            <strong>not</strong> arg/env variables, but they can be used to set those via
            substitution.
          </p>

          <p>You can only inject <strong>args</strong> via:</p>
          <ul>
            <li>command line (<code>--build-arg</code> with compose or normal docker CLI)</li>
            <li>
              compose file (which can inject the value from a .env file into the compose file using
              substitution)
            </li>
          </ul>

          <p>
            Injecting <em>runtime</em> <strong>env</strong> variables (<strong>remember:</strong>
            these values are not injected into the Dockerfile, which only accepts args):
          </p>
          <ul>
            <li>
              <code>env_file</code> defines env files that will export all their variables into the
              container runtime environment.
            </li>
            <li>
              <code>environment</code> attribute can also be used, but ideally you don't want to mix
              using the attribute and the env file (preferred). These values take precedence over
              env file values and are the same as passing them via the <code>-e</code> flag in a
              <code>docker run</code> command.
            </li>
            <li>
              Environment variables are also imported from the shell and have the
              <em>highest</em> precedence except for using <code>-e</code> to pass variables
              explicitly.
            </li>
          </ul>
        </section>

        <section style="border: 1px black solid; padding: 1rem">
          <h3>Quick variable re-rundown</h3>
          <ul>
            <li>
              <code>--env-file</code> and <code>.env</code>: substitute values into Docker compose
              files
            </li>
            <li>
              <code>env_file</code> and <code>environment</code> within Docker compose file (or
              <code>-e</code> with Docker non-compose CLI): injects env variables into container
              runtime
            </li>
            <li>
              <code>args</code> within Docker compose file (or <code>--build-arg</code> w/ CLI):
              injects build-only arguments into Dockerfiles
            </li>
            <li>
              <code>ARG</code> within Dockerfile: build-only argument that only exists while
              creating the image from the Dockerfile (so cannot be used in
              <code>RUN/ENTRYPOINT</code>)
            </li>
            <li>
              <code>ENV</code> within Dockerfile: Dockerfile build <em>and</em> container runtime
              variables that must be populated by Dockerfile <code>ARG</code>s (or hardcoded)
            </li>
          </ul>
        </section>

        <h3>Secrets</h3>
        <section>
          <p>
            Environment secrets <em>within</em> the container can just be passed via an environment
            variable at container create time. This is generally secure since the secret is only
            accessible from within your already secured host at container creation time. However, if
            you prefer, you can define a runtime secret under a service in the compose file.
          </p>

          <p>
            Since images consist of layers, even if you delete sensitive files, if they existed in a
            layer, they are accessible in that layer in the published image.
          </p>

          <p>There are two ways to avoid this issue:</p>
          <ul>
            <li>
              <strong>Recommended:</strong> <code>secret</code> attribute in the
              <em><code>build</code></em> section of a service in the compose file or option on the
              command line will set the value of the secret and make the secret available as a file
              during image buildtime via a temporary mount, which can be accessed like (path can be
              customized):
              <code>RUN --mount=type=secret,id=[secret_name] [SECRET]=$(cat /run/secrets/id)</code>
              <p>
                Note: in the compose file, secrets are declated in their own section and then
                referenced individually for each service that will use them, which reduces services
                having access to secrets they don't need.
              </p>
            </li>
            <li>
              <strong>Less recommended because brittle:</strong> Only the layers from the final
              stage in a multi-stage build are preserved in the built image (caution: layers from
              direct parent stages of the final stage will be present too), so you can perform
              sensitive operations in earlier stages, and then copy over the resulting files to the
              final stage to avoid exposing the sensitive data.
            </li>
          </ul>

          <h4>SSH</h4>
          <p>
            Use <code>RUN --mount=type=ssh</code> to use SSH temporary mounts if you need to perform
            SSH actions during image builds to use the host's SSH keys and not have to copy them
            into a layer.
          </p>
        </section>
      </section>

      <h2>Multi-stage Builds</h2>
      <section>
        <p>
          Mutiple <code>FROM</code> instructions can be used in a single Dockerfile to signify
          different stages. These stages can be descendants of earlier stages or not, and only the
          layers (and parent layers) of the final resulting built image are preserved.
        </p>

        <p>
          Files do NOT carry over between stages unless the stage calls
          <code>FROM [parent_stage]</code> to set itself as a descendant.
        </p>

        <p>
          Stages can be targeted via command line or compose file <code>target</code> to only build
          the stage specified along with its dependency stages.
        </p>
      </section>

      <h2>Prod vs Dev</h2>
      <section>
        <p>
          The most important usage of multi-stage builds is building for different environments.
          Docker compose makes this simple by <code>merging</code> compose files. You can define a
          base <code>compose.yaml</code> and then different environment compose files and pass them
          via the <code>-f</code> (files) option for <code>docker compose</code>.
        </p>

        <p>
          These different compose files can point to different <code>env_files</code> and build
          targets.
        </p>

        <p>
          You can also split a Dockerfile into multiple files if needed and pass them via the
          <code>-f</code> (files) option for <code>docker build</code>.
        </p>

        <p>
          Note: you usually shouldn't use Docker compose in production, but it is fine for smaller
          projects, and the compose file can be converted very simply into raw Docker commands.
        </p>

        <h4>Local Development</h4>
        <section>
          <p>
            While it is generally not recommended to put DBs and stateful frameworks into containers
            (since there is little benefit and managed DBs are the de facto standard), it is fine
            and often encouraged for local development to simulate the production environment as
            much as possible.
          </p>
          <p>Code auto-reload is supported in a few ways (from most to least recommended):</p>
          <ul>
            <li>
              <strong>Recommended:</strong> <code>develop</code> in Docker compose has options watch
              a certain <code>path</code> on your host (with optional
              <code>ignore</code> sub-paths), map it to a <code>target</code> path in the container,
              and perform actions on file change:
              <ul style="margin-top: 1rem">
                <li><code>watch</code> (copy changed files to container)</li>
                <li><code>rebuild</code> (rebuild image and create new container)</li>
                <li>
                  <code>sync+restart</code> (restarts container (i.e., reruns
                  <code>CMD</code> instruction) after copying files)
                </li>
              </ul>
            </li>
            <li>
              Bind mounts / host volumes map local paths to paths in the container, but these can
              have more performance issues and have less fine-grain control with only
              <code>.dockerignore</code> and sub-path ignoring.
            </li>
            <li>
              <strong>Not recommended:</strong> development from inside the container, which poses
              lots of DX annoyances of not being able to use most of your OS-installed tools (unless
              creating a likely bloaated development Dockerfile with tools, which all developers
              would have to use or ignore anyway)
            </li>
          </ul>
        </section>
      </section>

      <h2>Caching/Optimizations</h2>
      <section>
        <p>Docker caches come in three varieties:</p>
        <ul>
          <li>
            <h3>Builder cache</h3>
            <p>
              Order Dockerfile instructions such that the layers whose files change the least occur
              first. This is because the Docker builder caches each layer to optimize building
              successive images, and it will pull the image layers from its cache if possible.
            </p>
            <p>This also applies to multi-stage builds.</p>
            <p>
              Only <code>ADD/COPY</code> check for file modification time and size to see if files
              have changed/added/deleted. Other instructions are cache hits as long as the
              instruction text hasn't changed.
            </p>
            <ul>
              <li>
                Separate large composite commands if earlier parts can be cached more consistently.
              </li>
              <li>Batch commands together when they will always use the same cache files.</li>
              <li>
                Copy only the needed code (use <code>.dockerignore</code> or specify required
                directories to avoid cache invalidation)
              </li>
              <li>
                Secrets and results of <code>RUN</code> instructions are not checked for cache
                invalidation, so add a cache-bust arg that you can change when you need to
                invalidate the cache starting at a certain layer.
              </li>
            </ul>
          </li>
          <li>
            <h3>Cache mounts</h3>
            <p>
              Via the syntax
              <code>RUN --mount=type=cache,target=/go/pkg/mod/ [your command]</code> you can create
              a persistent cache volume that can be re-used between stages and builds. This cache
              isn't invalidated like builder layer caching, and simply acts as persistent storage
              that the command will be run from. Useful for dependency downloads since dependencies
              usually only change one at a time, and this way the package manager can confirm that
              all but that dependency is downloaded.
            </p>
          </li>
          <li>
            <h3>Cache storage backends</h3>
            <p>
              Using <code>cache-from/to</code> options on the <code>docker build</code> command, you
              can import/export the builder cache for that named image from/to a remote backend to
              preserve caching in stateless Docker environments, like CI/CD.
            </p>

            <p>
              This is especailly useful in multi-stage builds, especially those that don't have a
              parental stage chain:
            </p>
            <ol>
              <li>Use <code>target</code> to only build a specific stage of the build</li>
              <li>Use <code>cache-to/from</code> to cache each target</li>
              <li>
                Combine as many <code>cache-from</code> options to the <code>build</code> command as
                neeed to build the desired final target (also use <code>cache-to</code> to push the
                final image cache)
              </li>
            </ol>
          </li>
        </ul>
      </section>

      <h2>Debugging</h2>
      <section>
        <h3>Interactivity</h3>
        <section>
          <p>Note:</p>
          <ul>
            <li>
              <code>-i</code> means 'interactive' (connects stdin from the container), so
              <code>[cmd] | docker run -i</code> works
            </li>
            <li>
              <code>-t</code> means 'teletype' (connects the stderr/out/in in a pseudo-terminal),
              but there is no way to pass commands to stdin (due to reasons I don't quite
              understand)
            </li>
            <li>
              Setting no option will either take over your shell (in the case of
              <code>docker run</code>) or forward a command to a new container shell with no stream
              connection otherwise (in the case of <code>docker exec</code>)
            </li>
          </ul>
          <p>You usually want to use <code>-it</code> for a standard pseudo-terminal experience.</p>
        </section>

        <h3>Strategies</h3>
        <ul>
          <li>
            For running containers:
            <ul>
              <li>
                <p>
                  <code>docker attach [container_name]</code> attaches to the shell running the
                  container's process, forwarding stdout/err/in to/from your terminal.
                </p>
                <p>
                  <strong>Caution:</strong> by default, all received signal are forwarded to the
                  process (<code>--sig-proxy=true</code>), so <code>CTRL+C</code> will
                  <strong>kill</strong> the container. Because of this, it is recommended to use
                  <code>docker exec</code> or set <code>--sig-proxy=false</code>.
                </p>
                <p>
                  (NOTE: <code>docker run -it</code> before attaching will allow for
                  <code>CTRL+P CTRL+Q</code> to detach from the container without killing it.)
                </p>
              </li>
              <li>
                <code>docker exec -it [container_name] [command]</code> runs a command in a new
                shell in the container and opens an interactive pseudo-terminal in that shell.
                <code>/bin/sh</code> is the usual command for opening a shell session.
              </li>
            </ul>
          </li>
          <li>
            For stopped/stopping containers (also can be useful for running containers):
            <ul>
              <li>
                <code>docker logs [container_name]</code> will show the logs of the container
                (collected from stdout & stderr), even from a <em>stopped</em> container, so this
                can be useful for diagnosing the causes of a container crash.
                <p><code>-f</code> option allows for following the logs.</p>
              </li>
              <li>
                <code>docker cp [container_name:/file/path] [host/file/path]</code> copies files
                from the docker container to the host, which is useful for inspecting log files
              </li>
              <li>
                <code
                  >docker commit [container_name] [new_image_name] && docker run -it
                  [new_image_name] /bin/sh
                </code>
                commits the current state of the container to a new image and then spins up a new
                container from that image with a shell session. Useful for debugging the file state
                of a container that is crashing on startup by overwriting the
                <code>CMD</code> instruction.
                <p>
                  <strong>NOTE:</strong> this does not preserve env variables or anything that is
                  not a file state, so this usefulness is limited.
                </p>
              </li>
              <li>
                <p>
                  If you really need to restart the same container (to inspect env variables or
                  other non-file state) but with a different command to prevent crashing:
                </p>
                <ol>
                  <li>
                    edit <code>/var/lib/docker/containers/[container_id]/config.v2.json</code> (or
                    wherever your containers are stored) to change the <code>"Path"</code> parameter
                    to your new commands (usuaully <code>/bin/sh</code>)
                  </li>
                  <li>
                    <code>service docker restart</code> (linux command - just use Docker Desktop on
                    client) to restart the docker service (this will stop all running containers
                    unless you enable 'live-restore')
                  </li>
                  <li>
                    <code>docker start -ai [container_name]</code> to attach to the shell
                    immediately
                  </li>
                </ol>
              </li>
            </ul>
          </li>
        </ul>
      </section>
    </main>
  </body>
</html>
